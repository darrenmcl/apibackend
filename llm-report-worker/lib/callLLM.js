const axios = require('axios');
const logger = require('./logger'); // Assuming your logger is in the same lib directory

// Get configuration from environment variables
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OPENAI_BASE_URL = process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1';
const OPENROUTER_MODEL = process.env.OPENROUTER_MODEL || 'anthropic/claude-3.5-sonnet';

// Construct the full API URL
const OPENAI_API_URL = `${OPENAI_BASE_URL}/chat/completions`;

if (!OPENAI_API_KEY) {
  logger.error('[callLLM] FATAL: OPENAI_API_KEY environment variable is not set.');
  throw new Error('OPENAI_API_KEY is not configured.');
}

logger.info(`[callLLM] Initialized with base URL: ${OPENAI_BASE_URL}`);
logger.info(`[callLLM] Default model: ${OPENROUTER_MODEL}`);

/**
 * Calls an LLM provider (OpenAI or OpenRouter) to generate content.
 * @param {string} promptText - The main user prompt for the section.
 * @param {string} model - The model to use (e.g., 'openai/gpt-4-turbo' or 'anthropic/claude-3.5-sonnet').
 * @param {string} systemMessage - The system message/instruction for the LLM.
 * @returns {Promise<string>} - The content generated by the LLM.
 */
async function callLLM(promptText, model = null, systemMessage = null) {
  // Input validation
  if (!promptText || typeof promptText !== 'string') {
    logger.error(`[callLLM] Invalid promptText: ${promptText} (type: ${typeof promptText})`);
    return '';
  }
  
  // Default system message for professional reports
  if (!systemMessage || typeof systemMessage !== 'string') {
    systemMessage = `You are a professional business analyst writing industry reports. Write in a confident, authoritative tone as if you are an expert consultant. Never mention AI, language models, training data, knowledge cutoffs, or that you are an AI assistant. Write as if you are a human expert with current market knowledge. Focus on actionable insights and strategic recommendations. Use present tense and write with authority about current market conditions and trends.`;
  }
  
  // Use the provided model, or fall back to environment default, or final fallback
  let apiModel = model || OPENROUTER_MODEL;
  
  // If using OpenRouter, keep the full model name (e.g., 'anthropic/claude-3.5-sonnet')
  // If using OpenAI directly and model has 'openai/' prefix, strip it
  if (OPENAI_BASE_URL.includes('openai.com') && apiModel.startsWith('openai/')) {
    apiModel = apiModel.split('/')[1];
  }
  
  // Map common model names to OpenRouter format if using OpenRouter
  if (OPENAI_BASE_URL.includes('openrouter.ai')) {
    const modelMappings = {
      'gpt-4-turbo': 'openai/gpt-4-turbo',
      'gpt-4': 'openai/gpt-4',
      'gpt-3.5-turbo': 'openai/gpt-3.5-turbo',
      'claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',
      'claude-3-opus': 'anthropic/claude-3-opus-20240229',
      'claude-3-sonnet': 'anthropic/claude-3-sonnet-20240229'
    };
    
    if (modelMappings[apiModel]) {
      apiModel = modelMappings[apiModel];
    }
    
    // Ensure we have a provider prefix for OpenRouter
    if (!apiModel.includes('/')) {
      apiModel = OPENROUTER_MODEL; // Fall back to default
    }
  }

  const requestBody = {
    model: apiModel,
    messages: [
      {
        role: 'system',
        content: systemMessage
      },
      {
        role: 'user',
        content: promptText
      }
    ],
    temperature: 0.7,
    max_tokens: 2048,
  };

  // Add OpenRouter-specific headers if using OpenRouter
  const headers = {
    'Authorization': `Bearer ${OPENAI_API_KEY}`,
    'Content-Type': 'application/json'
  };

  if (OPENAI_BASE_URL.includes('openrouter.ai')) {
    headers['HTTP-Referer'] = process.env.OPENROUTER_REFERER || 'https://your-app.com';
    headers['X-Title'] = process.env.OPENROUTER_TITLE || 'LLM Report Worker';
  }

  logger.info(`[callLLM] Making API call to ${OPENAI_BASE_URL} with model: ${apiModel}`);
  
  // For debugging, log the request (remove in production)
  logger.debug('[callLLM] Request URL:', OPENAI_API_URL);
  logger.debug('[callLLM] Request Model:', apiModel);

  try {
    const response = await axios.post(OPENAI_API_URL, requestBody, {
      headers: headers,
      timeout: 60000 // 60 second timeout
    });

    if (response.data && response.data.choices && response.data.choices.length > 0) {
      const content = response.data.choices[0].message.content;
      logger.info(`[callLLM] Successfully received response from API for model: ${apiModel}`);
      return content.trim();
    } else {
      logger.warn('[callLLM] API response was successful but contained no choices.', response.data);
      return ''; 
    }
  } catch (error) {
    logger.error('[callLLM] Error calling LLM API.');
    
    if (error.response) {
      // The request was made and the server responded with a status code that falls out of the range of 2xx
      logger.error(`[callLLM] API Error Status: ${error.response.status}`);
      logger.error(`[callLLM] API Error Message: ${error.response.statusText}`);
      
      // Log the actual error details
      if (error.response.data) {
        logger.error('[callLLM] API Error Data:', JSON.stringify(error.response.data, null, 2));
        
        // Log specific OpenRouter/OpenAI error messages
        if (error.response.data.error) {
          logger.error(`[callLLM] Error Details: ${error.response.data.error.message || error.response.data.error}`);
        }
      }
      
      // Log request details for debugging
      logger.error(`[callLLM] Failed request - URL: ${OPENAI_API_URL}`);
      logger.error(`[callLLM] Failed request - Model: ${apiModel}`);
      logger.error(`[callLLM] Failed request - Base URL: ${OPENAI_BASE_URL}`);
      
    } else if (error.request) {
      logger.error('[callLLM] No response received from API:', error.request);
    } else {
      logger.error('[callLLM] Error setting up API request:', error.message);
    }
    
    // Return empty string on error so the process doesn't hang, but the error is logged.
    return '';
  }
}

module.exports = callLLM;
