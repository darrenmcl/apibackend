const axios = require('axios');
const logger = require('./logger'); // Assuming your logger is in the same lib directory

// Get the API key from environment variables
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions';

if (!OPENAI_API_KEY) {
  logger.error('[callLLM] FATAL: OPENAI_API_KEY environment variable is not set.');
  // Throwing an error here is better than letting it fail silently later
  throw new Error('OPENAI_API_KEY is not configured.');
}

/**
 * Calls an LLM provider (OpenAI) to generate content.
 * @param {string} promptText - The main user prompt for the section.
 * @param {string} model - The model to use (e.g., 'openai/gpt-4-turbo').
 * @param {string} systemMessage - The system message/instruction for the LLM.
 * @returns {Promise<string>} - The content generated by the LLM.
 */
async function callLLM(promptText, model = 'gpt-4-turbo', systemMessage = 'You are a helpful assistant.') {
  // The model name might come in as 'openai/gpt-4-turbo', let's strip the prefix for the API call.
  const apiModel = model.startsWith('openai/') ? model.split('/')[1] : model;

  const requestBody = {
    model: apiModel,
    messages: [
      {
        role: 'system',
        content: systemMessage
      },
      {
        role: 'user',
        content: promptText
      }
    ],
    temperature: 0.7, // Adjust temperature for more or less creative responses
    max_tokens: 2048, // Adjust max tokens based on expected length of sections
  };

  logger.info(`[callLLM] Making API call to OpenAI with model: ${apiModel}`);
  // For deep debugging, you can log the full request body, but be mindful of token usage and cost.
  // logger.debug('[callLLM] Request Body:', JSON.stringify(requestBody, null, 2));

  try {
    const response = await axios.post(OPENAI_API_URL, requestBody, {
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json'
      }
    });

    if (response.data && response.data.choices && response.data.choices.length > 0) {
      const content = response.data.choices[0].message.content;
      logger.info(`[callLLM] Successfully received response from OpenAI for model: ${apiModel}`);
      return content.trim();
    } else {
      logger.warn('[callLLM] OpenAI API response was successful but contained no choices.', response.data);
      return ''; // Return empty string if no content was generated
    }
  } catch (error) {
    logger.error('[callLLM] Error calling OpenAI API.');
    if (error.response) {
      // The request was made and the server responded with a status code that falls out of the range of 2xx
      logger.error(`[callLLM] API Error Status: ${error.response.status}`);
      logger.error('[callLLM] API Error Data:', JSON.stringify(error.response.data, null, 2));
    } else if (error.request) {
      // The request was made but no response was received
      logger.error('[callLLM] No response received from OpenAI API:', error.request);
    } else {
      // Something happened in setting up the request that triggered an Error
      logger.error('[callLLM] Error setting up API request:', error.message);
    }
    // Return empty string on error so the process doesn't hang, but the error is logged.
    return '';
  }
}

module.exports = callLLM;
